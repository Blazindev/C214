{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6WNEh4zEfMJVr2Yh+8SuA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blazindev/C214/blob/main/DesafioTecnico_GabrielDiniz_Dhauz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mysql-connector-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJiBsPpTINRy",
        "outputId": "121f2a0b-386b-4baf-d3f6-3826279dde7b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mysql-connector-python in /usr/local/lib/python3.10/dist-packages (8.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "id": "FBmxR-vUhXRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from decimal import Decimal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "Z-GCUJXoKdJs",
        "outputId": "bdaa1916-2f3b-4395-ce9e-0ffed971f5af"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-88-a15486c16467>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-88-a15486c16467>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install pandas\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQGLvhitHofs",
        "outputId": "05dd3274-7f31-4a5d-95dc-dea82c77480b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conexão ao banco de dados MySQL bem-sucedida!\n"
          ]
        }
      ],
      "source": [
        "import mysql.connector\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "config = {\n",
        "  'user': 'candidate_user',\n",
        "  'password': 'D3@bGh664%$1VHv*',\n",
        "  'host': 'dhauz-instance.cutloqirhpd7.us-east-1.rds.amazonaws.com',\n",
        "  'database': 'db_hiring_test'\n",
        "}\n",
        "\n",
        "try:\n",
        "    conn = mysql.connector.connect(**config)\n",
        "    print(\"Conexão ao banco de dados MySQL bem-sucedida!\")\n",
        "except mysql.connector.Error as err:\n",
        "    print(f\"Erro ao conectar ao banco de dados: {err}\")\n",
        "    exit(1)\n",
        "\n",
        "try:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\"SELECT * FROM db_hiring_test.raw_transactions_table\")\n",
        "    transactions_table = cursor.fetchall()\n",
        "    df_transactions_table = pd.DataFrame(transactions_table, columns=cursor.column_names)\n",
        "\n",
        "    cursor.execute(\"SELECT * FROM db_hiring_test.raw_transactions_fee\")\n",
        "    transactions_fee = cursor.fetchall()\n",
        "    df_transactions_fee = pd.DataFrame(transactions_fee, columns=cursor.column_names)\n",
        "\n",
        "\n",
        "except mysql.connector.Error as err:\n",
        "    print(f\"Erro ao executar a consulta: {err}\")\n",
        "\n",
        "finally:\n",
        "    cursor.close()\n",
        "    conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fase 1 – Análise sobre as transações:\n",
        "\n",
        "# 1. Qual é o endereço (carteira) com maior volume de transações enviadas?\n",
        "endereco_maior_volume_enviado = df_transactions_table.groupby('AddressOrigin')['TotalSent'].sum()\n",
        "df_transactions_table['TotalSent'] = pd.to_numeric(df_transactions_table['TotalSent'], errors='coerce')\n",
        "df_transactions_table = df_transactions_table.dropna(subset=['TotalSent'])\n",
        "endereco_maior_volume_enviado = endereco_maior_volume_enviado.idxmax()\n",
        "print(endereco_maior_volume_enviado)\n",
        "\n",
        "# 2. Qual é o dia do mês com maior volume de transações realizadas?\n",
        "df_transactions_table['dia_do_mes'] = pd.to_datetime(df_transactions_table['SentDate'])\n",
        "dia_maior_volume_transacoes = df_transactions_table.groupby('SentDate')['TotalSent'].sum()\n",
        "dia_maior_volume_transacoes = dia_maior_volume_transacoes.idxmax()\n",
        "print(dia_maior_volume_transacoes)\n",
        "\n",
        "# 3. Em qual dia da semana geralmente mais transações são realizadas?\n",
        "df_transactions_table['SentDate'] = pd.to_datetime(df_transactions_table['SentDate'])\n",
        "df_transactions_table['dia_da_semana'] = df_transactions_table['SentDate'].dt.day_name()\n",
        "grupo_dia_semana = df_transactions_table.groupby('dia_da_semana')['TotalSent'].sum()\n",
        "dia_semana_maior_volume = grupo_dia_semana.idxmax()\n",
        "print(dia_semana_maior_volume)\n",
        "\n",
        "#4.Quais transações possuem condições atípicas e precisam ser validadas com o time responsável pela disponibilização dos dados?\n",
        "# Calcular o primeiro e terceiro quartis\n",
        "Q1 = df_transactions_table['TotalSent'].quantile(0.25)\n",
        "Q3 = df_transactions_table['TotalSent'].quantile(0.75)\n",
        "# Calcular IQR\n",
        "IQR = Q3 - Q1\n",
        "limite_inferior = Q1 - 1.5 * IQR\n",
        "limite_superior = Q3 + 1.5 * IQR\n",
        "transacoes_atipicas = df_transactions_table[(df_transactions_table['TotalSent'] < limite_inferior) | (df_transactions_table['TotalSent'] > limite_superior)]\n",
        "if not transacoes_atipicas.empty:\n",
        "    print(\"Transações atípicas:\")\n",
        "    print(transacoes_atipicas)\n",
        "else:\n",
        "    print(\"Não foram encontradas transações atípicas.\")\n",
        "\n",
        "#5.Qual a carteira com o maior saldo final? (considere que todas as carteiras estejam zeradas no início das análises e que seja possível existir saldo negativo).\n",
        "saldo_final_por_carteira = df_transactions_table.groupby('AddressDestination')['TotalSent'].sum()\n",
        "carteira_maior_saldo_final = saldo_final_por_carteira.idxmax()\n",
        "print(\"A carteira com o maior saldo final é:\", carteira_maior_saldo_final)\n"
      ],
      "metadata": {
        "id": "CMu1JvC9KxEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c98381-3772-4bd0-8346-67ae6884b84f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A-42\n",
            "2021-01-23 16:05:49\n",
            "Saturday\n",
            "Não foram encontradas transações atípicas.\n",
            "A carteira com o maior saldo final é: A-49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fase 2 – Análise sobre as taxas:\n",
        "\n",
        "#1.Considerando que a carteira origem é responsável por pagar as taxas de envio, qual carteira seria responsável pelo maior pagamento de taxas em janeiro de 2021?\n",
        "df_transactions_table['SentDate'] = pd.to_datetime(df_transactions_table['SentDate'])\n",
        "transactions_jan_2021 = df_transactions_table[(df_transactions_table['SentDate'] >= '2021-01-01') & (df_transactions_table['SentDate'] <= '2021-01-31')]\n",
        "\n",
        "fees_paid_by_wallet = transactions_jan_2021.groupby('AddressOrigin')['TotalSent'].sum() * df_transactions_fee['fee-percentage']\n",
        "fees_paid_by_wallet = fees_paid_by_wallet.fillna(0)\n",
        "\n",
        "wallet_with_highest_fees = fees_paid_by_wallet.idxmax()\n",
        "print(\"Carteira responsável pelo maior pagamento de taxas em janeiro de 2021:\", wallet_with_highest_fees)\n",
        "\n",
        "#2.E em fevereiro de 2021?\n",
        "df_transactions_table['SentDate'] = pd.to_datetime(df_transactions_table['SentDate'])\n",
        "\n",
        "transactions_feb_2021 = df_transactions_table[(df_transactions_table['SentDate'] >= '2021-02-01') & (df_transactions_table['SentDate'] <= '2021-02-28')]\n",
        "\n",
        "if transactions_feb_2021['TotalSent'].isnull().all():\n",
        "    print(\"Todos os valores na coluna para o mês 'TotalSent' são NaN. Não é possível calcular o pagamento de taxas.\")\n",
        "else:\n",
        "    fees_paid_by_wallet_feb = transactions_feb_2021.groupby('AddressOrigin')['TotalSent'].sum() * df_transactions_fee['fee-percentage']\n",
        "    fees_paid_by_wallet_feb = fees_paid_by_wallet_feb.fillna(0)\n",
        "    wallet_with_highest_fees = fees_paid_by_wallet.idxmax()\n",
        "    print(\"Carteira responsável pelo maior pagamento de taxas em fevereiro de 2021:\", wallet_with_highest_fees)\n",
        "\n",
        "#3.Qual é o id da transação com a maior taxa paga?\n",
        "def find_fee(total_sent):\n",
        "    total_sent = Decimal(str(total_sent))\n",
        "    for index, row in df_transactions_fee.iterrows():\n",
        "        if total_sent >= row['range-start'] and total_sent <= row['range-end']:\n",
        "            return float(row['fee-percentage'])\n",
        "    return None\n",
        "\n",
        "# Criando a nova coluna 'total_sent_fee' no DataFrame 'df_transactions_table'\n",
        "df_transactions_table['total_sent_fee'] = df_transactions_table['TotalSent'].apply(lambda x: x * find_fee(x) if pd.notna(x) else None)\n",
        "transaction_with_highest_fee_id = df_transactions_table.loc[df_transactions_table['total_sent_fee'].idxmax(), 'IdTransaction']\n",
        "print(\"ID da transação com a maior taxa paga:\", transaction_with_highest_fee_id)\n",
        "\n",
        "\n",
        "#4 Calculando a média de taxa paga considerando todas as transações realizadas\n",
        "mean_fee = df_transactions_table['total_sent_fee'].mean()\n",
        "print(\"A média de taxa paga considerando todas as transações realizadas é:\", mean_fee)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MsnPKdCPgrK",
        "outputId": "463e32d2-384b-43e6-ed56-4c82733cc733"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carteira responsável pelo maior pagamento de taxas em janeiro de 2021: A-42\n",
            "Todos os valores na coluna para o mês 'TotalSent' são NaN. Não é possível calcular o pagamento de taxas.\n",
            "ID da transação com a maior taxa paga: ID1293\n",
            "A média de taxa paga considerando todas as transações realizadas é: 6410.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fase 3 - Arquitetura:\n",
        "\n",
        "'''\n",
        "Para montar uma arquitetura na AWS para realizar as tarefas mencionadas (ingestão de dados por banco, garantia de qualidade dos dados, modelagem de dados, armazenamento e disponibilização dos dados), podemos utilizar:\n",
        "O AWS Glue para realizar a ingestão de dados e para todo ETL e orquestração do pipeline. O Data Quality poderia ser feito utilizando serviços como AWS Glue DataBrew ou AWS Glue ETL Jobs.\n",
        "Poderiamos usar Amazon Redshift para armazenar e modelar os dados. Para monitoramento, utilizaríamos o Amazon CloudWatch para monitorar métricas operacionais e configurar alertas sobre eventos importantes.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "WU91iqHXfO8D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}